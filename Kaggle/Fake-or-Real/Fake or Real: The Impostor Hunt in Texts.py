# -*- coding: utf-8 -*-
"""notebookcc02f1c748

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/saharhaimyaccov/notebookcc02f1c748.36524087-96df-43cb-9fad-c10e9d64ae74.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250802/auto/storage/goog4_request%26X-Goog-Date%3D20250802T131028Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D36f01e394c4e364a0ff8b0c633ae5a24fe168a26809e898adaf2a22a0508342fabc38882806d4d3d7b4c25de4728bafc440261dc3ca8a9cf197bb5cffb4098273c7ce8a3cde69390568819dfd3f3ac5e31ce470ae7347b0412fc5c722b4838fd7012b16ca6f872ac757e813eb0abdd6f301e302f03995e4d8650c061bb4d0f52b714b3c84ab5b434fd514a0e500309d85fbf8eb37f4933734e42397e061e72547be5427db14711c77bb808ca46c65cd6a267aaa0fabe555918a9e37ccfe80795124f0cecaab312f8943fe175356c23a2f485e7f07922128dd6a3527762e0649e9a32312798f64b94bee75adda178dc36335f38d7e35518e1add74dd62caf3990
"""

# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE
# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.
import kagglehub
kagglehub.login()

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

fake_or_real_the_impostor_hunt_path = kagglehub.competition_download('fake-or-real-the-impostor-hunt')

print('Data source import complete.')

import numpy as np
import pandas as pd

# load file
labels_df = pd.read_csv('/kaggle/input/fake-or-real-the-impostor-hunt/data/train.csv')

text_true = []
text_fake = []

# for any article in file
for i, row in labels_df.iterrows():
    article_id = row['id']  # לדוגמה: article_0000
    real_id = row['real_text_id']   # 1 או 2
    str_article_id = 'article_' + str(i).zfill(4)  # ==> 'article_0005'

    #the pathes pf  articales
    file1_path = f'/kaggle/input/fake-or-real-the-impostor-hunt/data/train/{str_article_id}/file_1.txt'
    file2_path = f'/kaggle/input/fake-or-real-the-impostor-hunt/data/train/{str_article_id}/file_2.txt'


    #read texts
    with open(file1_path, 'r', encoding='utf-8') as f1:
        text1 = f1.read()

    with open(file2_path, 'r', encoding='utf-8') as f2:
        text2 = f2.read()

    #save file by true or fake
    if real_id == 1:
        text_true.append(text1)
        text_fake.append(text2)
    else:
        text_true.append(text2)
        text_fake.append(text1)

    # 94 files exists
    if i == 94:
        break

print(f"🟢 Real texts count: {len(text_true)}")
print(f"🔴 Fake texts count: {len(text_fake)}")

import re

def clean_and_split(text):
    #Return lowercase words from text without punctuation.
    return re.findall(r'\b\w+\b', text.lower())

zero_pair_text_in_file = []
min_len = min(len(text_true), len(text_fake))


for i in range(len(text_true)):
    true_words = clean_and_split(text_true[i])
    fake_words = clean_and_split(text_fake[i])
    both_words = [word for word in true_words if word in fake_words]
    if (len(both_words) == 0 ):
        zero_pair_text_in_file.append(i+1)
        print(f'🔎 Pair {i+1}: {len(both_words)}' )


if zero_pair_text_in_file:
    print(f"\n😶 No common words found in {len(zero_pair_text_in_file)} pairs: {zero_pair_text_in_file}")
else:
    print("\n🎉 All pairs have at least one common word!")

df = pd.DataFrame({'True': text_true,'Fake': text_fake})

df.head()

from wordcloud import WordCloud
import matplotlib.pyplot as plt

def generate_wordcloud(text):
    """
    Generates and displays a word cloud from the given text.

    Args:
        text (str): The input text to visualize.
    """
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title("☁️ Word Cloud", fontsize=16)
    plt.show()

for num_df_file in [0,1]:
    if num_df_file == 0:
        print(f"🚀 {'-'*20} detect for True file {'-'*20}")
    else:
        print(f"🚨 {'-'*20} detect for Fake file {'-'*20}")
    for i in range(10):
        print(f'⏱️ file {i}')
        sample_text = df.iloc[i, num_df_file]  # שורה i, עמודה 0
        print(f"🌟 Word Cloud for row {i+1}")
        generate_wordcloud(sample_text)

from sklearn.feature_extraction.text import TfidfVectorizer
from joblib import dump, load
def create_vector_from_list(col_values,vectorizer_file_name):
    # create vector from texts
    vectorizer = TfidfVectorizer(stop_words='english')  # auto stop word
    X = vectorizer.fit_transform(col_values)
    X = X.toarray()
    dump(vectorizer, f'{vectorizer_file_name}.joblib')
    return X ,vectorizer,vectorizer.get_feature_names_out()

all_texts = list(text_true) + list(text_fake)  # כל הטקסטים ביחד

vectors_all, vectorizer_all, feature_names_all = create_vector_from_list(all_texts , 'vectorizer_all_1')
vectors_t = vectors_all[:len(text_true), :]
vectors_f = vectors_all[len(text_true):, :]

print(len(vectorizer_all.get_feature_names_out()))

df_all_txt = pd.DataFrame(vectors_all)
print(f"🧐 Total missing values (NaN) in dataframe: {df_all_txt.isnull().sum().sum()} ❌")

print(df_all_txt.head(2))

print(df_all_txt.shape)  # התוצאה: (rows, columns)

df_all_txt.describe()

df_vec_t_f = df_all_txt
print(f"🔎 Total missing values (NaN) in dataframe: {df_vec_t_f.isnull().sum().sum()} ❌")


df_vectors_t_mean = pd.DataFrame(np.mean(vectors_t, axis=1), columns=['mean_vector_t'])
df_vectors_t_std = pd.DataFrame(np.std(vectors_t, axis=1), columns=['std_vector_t'])
df_vectors_f_mean = pd.DataFrame(np.mean(vectors_f, axis=1), columns=['mean_vector_f'])
df_vectors_f_std = pd.DataFrame(np.std(vectors_t, axis=1), columns=['std_vector_f'])

df_true = pd.DataFrame({
    'mean_vector': df_vectors_t_mean['mean_vector_t'],
    'std_vector': df_vectors_t_std['std_vector_t'],
    'label': 'true'
})

df_fake = pd.DataFrame({
    'mean_vector': df_vectors_f_mean['mean_vector_f'],
    'std_vector': df_vectors_f_std['std_vector_f'],
    'label': 'fake'
})

df_vec_t_f_mean = pd.concat([df_true, df_fake], axis=0).reset_index(drop=True)


print(f"🧐 Total missing values (NaN) in dataframe mean: {df_vec_t_f.isnull().sum().sum()} ❌")
print(df_vec_t_f_mean.iloc[[20,25,34, 40, 102,112,122, 129]])

from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=2, n_init=10, random_state=42)
kmeans.fit(df_all_txt)
df_vec_t_f['cluster'] = kmeans.labels_

print(df_vec_t_f.shape)

print("Number of iterations run:", kmeans.n_iter_)
print("Number of clusters:", kmeans.n_clusters)
print("Random state:", kmeans.random_state)


import matplotlib.pyplot as plt

def plot_clusters(df, kmeans, normalize=True):
    """
    Plot the clustered data points in 2D with cluster centers.

    Args:
        df (pd.DataFrame): Original DataFrame of features.
        kmeans: Fitted KMeans model.
        normalize (bool): Whether the data was normalized (to plot in scaled space).
    """
    if normalize:
        from sklearn.preprocessing import StandardScaler
        scaler = StandardScaler()
        X = scaler.fit_transform(df)
    else:
        X = df.values

    labels = kmeans.labels_
    centers = kmeans.cluster_centers_

    plt.figure(figsize=(8,6))
    scatter = plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', alpha=0.6)
    plt.scatter(centers[:, 0], centers[:, 1], c='red', marker='X', s=200, label='Centers')

    plt.title('KMeans Clusters')
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.legend()
    plt.show()

from sklearn.decomposition import PCA

# צמצום למרחב דו-ממדי
pca = PCA(n_components=2)
reduced_vectors = pca.fit_transform(vectors_no_cluster)

# ואז עושים clustering על reduced_vectors

# שימוש לדוגמה:
plot_clusters(new_df, kmeans)
print("Cluster Centers Coordinates:")
print(kmeans.cluster_centers_)

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

def train_and_evaluate_random_forest(X, y, test_size=0.2, random_state=42):
    """
    מאמן מודל RandomForestClassifier ומחזיר את דיוק המודל על קבוצת הבדיקה

    פרמטרים:
    - X: numpy array או DataFrame של מאפיינים
    - y: numpy array או סדרה של תוויות (מספריות)
    - test_size: חלק הנתונים שישמש לבדיקה (ברירת מחדל 0.2)
    - random_state: מספר לזרע אקראי (ברירת מחדל 42)

    מחזיר:
    - accuracy: דיוק המודל על קבוצת הבדיקה (float)
    """
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state, stratify=y
    )
    model = RandomForestClassifier(n_estimators=100, random_state=random_state)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    return accuracy

from xgboost import XGBClassifier

def train_xgboost(X, y, test_size=0.2, random_state=42):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)
    model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=random_state)
    model.fit(X_train, y_train)
    return accuracy_score(y_test, model.predict(X_test))

from lightgbm import LGBMClassifier

def train_lightgbm(X, y, test_size=0.2, random_state=42):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)
    model = LGBMClassifier(random_state=random_state)
    model.fit(X_train, y_train)
    return accuracy_score(y_test, model.predict(X_test))

from sklearn.ensemble import ExtraTreesClassifier

def train_extra_trees(X, y, test_size=0.2, random_state=42):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)
    model = ExtraTreesClassifier(n_estimators=100, random_state=random_state)
    model.fit(X_train, y_train)
    return accuracy_score(y_test, model.predict(X_test))

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam

def train_neural_net(X, y, test_size=0.2, random_state=42, epochs=20, batch_size=32):
    import numpy as np
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import accuracy_score

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)

    model = Sequential([
        Dense(64, activation='relu', input_shape=(X.shape[1],)),
        Dense(32, activation='tanh'),
        Dense(1, activation='sigmoid')
    ])

    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])
    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)
    y_pred = (model.predict(X_test) > 0.5).astype(int)
    return accuracy_score(y_test, y_pred)

from sklearn.preprocessing import LabelEncoder
X = df_vec_t_f_mean.drop(columns = 'label')
y = df_vec_t_f_mean['label']
# יצירת מקודד והמרה
le = LabelEncoder()
y_encoded = le.fit_transform(y)

accuracy = train_lightgbm(X, y_encoded)
print(f"🎯 Cross-validated Accuracy: {accuracy:.4f}")

import numpy as np
import pandas as pd
from scipy.stats import entropy


def sigmoid_entropy_like(arr):
    s = np.sum(arr)
    # חישוב סיגמואיד: 1 / (1 + e^(-s))
    return 1 / (1 + np.exp(-s))


# חישוב אנטרופיה לכל שורה במטריצה (ווקטורים טקסט אמיתיים)
entropies_t = np.array([sigmoid_entropy_like(row) for row in vectors_t])

df_vectors_t_stats = pd.DataFrame({
    'mean_vector': np.mean(vectors_t, axis=1),
    'std_vector': np.std(vectors_t, axis=1),
    'var_vector': np.var(vectors_t, axis=1),
    'median_vector': np.median(vectors_t, axis=1),
    'entropy_vector': entropies_t,
    'label': 'true'
})

# חישוב אנטרופיה לכל שורה במטריצה (ווקטורים טקסט מזויפים)
entropies_f = np.array([sigmoid_entropy_like(row) for row in vectors_f])

df_vectors_f_stats = pd.DataFrame({
    'mean_vector': np.mean(vectors_f, axis=1),
    'std_vector': np.std(vectors_f, axis=1),
    'var_vector': np.var(vectors_f, axis=1),
    'median_vector': np.median(vectors_f, axis=1),
    'entropy_vector': entropies_f,
    'label': 'fake'
})

# איחוד טבלאות True ו-Fake
df_stats = pd.concat([df_vectors_t_stats, df_vectors_f_stats], axis=0).reset_index(drop=True)

print(df_stats.head())

from itertools import combinations

metrics = [
    'mean_vector',    # ממוצע
    'std_vector',     # סטיית תקן
    'median_vector',  # חציון
    'var_vector',     # שונות
    'entropy_vector'
]
label_col = 'label'

all_combinations = []

for r in range(1, len(metrics)+1):  # 1 עד כל המדדים
    combs = list(combinations(metrics, r))
    for comb in combs:
        cols = list(comb) + [label_col]
        df_subset = df_stats[cols]
        all_combinations.append((comb, df_subset))

for i in all_combinations:
    print(i[0])

df_stats

# הגדרת הפונקציות לכל מודל
model_funcs = {
    'RandomForest': train_and_evaluate_random_forest,
    'XGBoost': train_xgboost,
    'LightGBM': train_lightgbm,
    'ExtraTrees': train_extra_trees,
    'NeuralNetwork':train_neural_net,
}

# מאגר התוצאות הסופי
overall_results = {}

# בחירת פיצ'רים
selected_features = ['mean_vector'	,'std_vector'	,'var_vector',	'median_vector'	,'entropy_vector']  # תוכל להוסיף: 'vector_variance', וכו'

# הכנת X ו-y
X = df_stats[selected_features]
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(df_stats['label'])  # עכשיו y יהיה [0, 1]

# הרצת כל מודל פעם אחת בלבד
for model_name, func in model_funcs.items():
    print(f"\n🚀 Evaluating model: {model_name}")

    # הרצת המודל
    accuracy = func(X, y)  # הפונקציה שלך מחזירה את הדיוק (float)

    # שמירת תוצאה
    overall_results[model_name] = {
        'accuracy': accuracy,
        'features': selected_features
    }

    print(f"✅ Model: {model_name} | Accuracy: {accuracy:.4f}")

# סיכום כולל
print("\n🎯 Summary of Results:")
for model_name, result in overall_results.items():
    print(f"{model_name}: Accuracy = {result['accuracy']:.4f} using features {result['features']}")

def get_df_for_combination(all_combinations, target_comb):
    for comb_columns, df_subset in all_combinations:
        if comb_columns == target_comb:
            return df_subset
    return None

target_features = ('mean_vector'	,'std_vector'	,	'median_vector','var_vector','entropy_vector'	) # תוכל להוסיף: 'vector_variance', וכו'
df_target = get_df_for_combination(all_combinations, target_features)

df_target

!pip install optuna

import optuna
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score
import numpy as np

def optimize_random_forest_with_optuna(X, y, n_trials=30):
    """
    אופטימיזציית RandomForest בעזרת Optuna – על כל הדאטה (בלי ולידציה חיצונית).

    פרמטרים:
    - X: מאפיינים
    - y: תוויות
    - n_trials: מספר ניסיונות (חיפושי פרמטרים)

    מחזיר:
    - best_model: המודל עם הפרמטרים הטובים ביותר
    - best_params: הפרמטרים
    - best_score: הדיוק הממוצע
    """

    def objective(trial):
        # הצעות פרמטרים
        n_estimators = trial.suggest_int("n_estimators", 100, 300)
        max_depth = trial.suggest_int("max_depth", 5, 11)
        max_features = trial.suggest_categorical("max_features", ["sqrt", "log2", None])

        model = RandomForestClassifier(
            n_estimators=n_estimators,
            max_depth=max_depth,
            max_features=max_features,
            random_state=42,
            n_jobs=-1
        )

        # דיוק על כל הדאטה (אפשר להחליף ל-cross_val_score)
        model.fit(X, y)
        preds = model.predict(X)
        acc = accuracy_score(y, preds)
        return acc  # דיוק גבוה יותר = טוב יותר

    study = optuna.create_study(direction="maximize")
    study.optimize(objective, n_trials=n_trials)

    best_params = study.best_params
    best_score = study.best_value

    # אימון המודל הסופי עם הפרמטרים הטובים ביותר
    best_model = RandomForestClassifier(**best_params, random_state=42, n_jobs=-1)
    best_model.fit(X, y)

    return best_model, best_params, best_score

# יצירת X ו-y (נניח שכבר שטחת וקטורים)


# הרצת Optuna
best_model, best_params, best_acc = optimize_random_forest_with_optuna(X, y, n_trials=40)

print("🏆 Best RandomForest Model:")
print(f"Params: {best_params}")
print(f"Accuracy: {best_acc:.4f}")

from pprint import pprint
pprint(best_model.get_params())
print('-'*70)
import numpy as np
print(f"Most used feature index: {np.argmax(best_model.feature_importances_)}")
print('-'*70)
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.bar(range(len(best_model.feature_importances_)), best_model.feature_importances_)
plt.title("Feature Importances")
plt.xlabel("Feature Index")
plt.ylabel("Importance")
plt.show()
import joblib
joblib.dump(best_model, "best_random_forest_model.pkl")

import os

folder_path = '/kaggle/input/fake-or-real-the-impostor-hunt/data/test'
lab = {}

for root, dirs, files in os.walk(folder_path):
    # שולף את שם התקיה מתוך הנתיב (התקיה הנוכחית)
    folder_name = os.path.basename(root)

    for filename in files:
        file_path = os.path.join(root, filename)
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # אם המפתח קיים, מוסיף לרשימה, אחרת יוצר רשימה חדשה
        if folder_name not in lab:
            lab[folder_name] = []
        lab[folder_name].append(content)

import numpy as np
import pandas as pd

# שם המאמר לבדיקה
article_id = 'article_0000'

# תכונות שנבחרו עבור המודל
target_features = selected_features

# הדפסת מספר הטקסטים במאמר
num_texts = len(lab[article_id])
print(f"Number of texts in {article_id}: {num_texts}")

# שליפת שני הטקסטים הראשונים
text1 = lab[article_id][0]
text2 = lab[article_id][1]

# יצירת וקטורים מהטקסטים
vector1, _, _ = create_vector_from_list([text1],'vectorizer_txt1')
vector2, _, _ = create_vector_from_list([text2],'vectorizer_txt2')

# התאמת מבנה חד-ממדי
vec1 = vector1 if len(vector1.shape) == 1 else vector1[0]
vec2 = vector2 if len(vector2.shape) == 1 else vector2[0]

# חישוב תכונות לטקסט הראשון
features1 = {
    'mean_vector': np.mean(vec1),
    'std_vector': np.std(vec1),
    'median_vector': np.median(vec1),
    'var_vector': np.var(vec1),
    'entropy_vector': sigmoid_entropy_like(vec1)
}

# חישוב תכונות לטקסט השני
features2 = {
    'mean_vector': np.mean(vec2),
    'std_vector': np.std(vec2),
    'median_vector': np.median(vec2),
    'var_vector': np.var(vec2),
    'entropy_vector': sigmoid_entropy_like(vec2)
}

# יצירת טבלאות נתונים
df1 = pd.DataFrame([features1])
df2 = pd.DataFrame([features2])

# חיזוי עבור טקסט ראשון
prob1 = best_model.predict_proba(df1[target_features])[0][1]
pred1 = int(prob1 > 0.5)

# חיזוי עבור טקסט שני
prob2 = best_model.predict_proba(df2[target_features])[0][1]
pred2 = int(prob2 > 0.5)

# הדפסת תוצאות
print(f"\nText 1: Prediction probability: {prob1:.4f} | Binary prediction: {pred1}")
print(f"Text 2: Prediction probability: {prob2:.4f} | Binary prediction: {pred2}")

print(len(vectorizer_all.get_feature_names_out()))

from joblib import load
import numpy as np
from sklearn.base import ClusterMixin

def predict_and_distance(text: str, vectorizer: str, model: ClusterMixin) -> tuple[int, float]:
    """
    Predict cluster and compute Euclidean distance from cluster center for a given text.

    Args:
        text (str): Input text to classify.
        vectorizer_path (str): Path to saved vectorizer (.joblib).
        model (ClusterMixin): Pre-trained clustering model (e.g., KMeans).

    Returns:
        tuple[int, float]: (predicted_cluster, distance_to_cluster_center)
    """

    vec = vectorizer.transform([text])

    cluster = model.predict(vec)[0]
    center = model.cluster_centers_[cluster]
    distance = np.linalg.norm(vec.toarray() - center)

    return cluster, distance

# טעינת המודל פעם אחת מחוץ לפונקציה (למשל בתחילת הסקריפט)
# from joblib import load
# k_mean_model_1 = load('kmeans_model.joblib

# שימוש
cluster_id, dist = predict_and_distance(text1, vectorizer_all, kmeans)
cluster_id2, dist2 = predict_and_distance(text2, vectorizer_all, kmeans)

print(f"Cluster: {cluster_id}")
print(f"Distance from center: {dist:.6f}")
print(f"Cluster: {cluster_id2}")
print(f"Distance from center: {dist2:.6f}")
if dist>dist2:
    print('📘 text file 1 article 1 is Real !')
    print('📕 text file 1 article 2 is Fake !')
else:
    print('📕 text file 1 article 2 is Real !')
    print('📘 text file 1 article 1 is Fake !')

results = []

for article_id, texts in lab.items():
    for idx, text in enumerate(texts):
        try:
            # יצירת וקטור טקסט עם ה-vectorizer ששמרת (לדוגמה)
            vec = vectorizer_all.transform([text])

            # חיזוי אשכול עם מודל KMeans
            cluster = kmeans.predict(vec)[0]

            # חישוב מרחק אוקלידי למרכז האשכול
            center = kmeans.cluster_centers_[cluster]
            distance = np.linalg.norm(vec.toarray() - center)

            # הוספת תוצאה
            results.append({
                'article_id': article_id,
                'file_index': idx + 1,
                'cluster': cluster,
                'distance_to_center': distance
            })

        except Exception as e:
            print(f"Error processing article {article_id}, file_index {idx + 1}: {e}")
            # במקרה של שגיאה – אפשר לשים ערכים ברירת מחדל
            results.append({
                'article_id': article_id,
                'file_index': idx + 1,
                'cluster': -1,
                'distance_to_center': np.nan
            })

# יצירת DataFrame סופי
final_results_df = pd.DataFrame(results)
print(final_results_df.head())

final_results_df

def find_zero_one_pairs(df):
    zero_one_pairs = []

    grouped = df.groupby('article_id')
    for article_id, group in grouped:
        group = group.sort_values('file_index').reset_index(drop=True)
        for i in range(len(group) - 1):
            row1 = group.loc[i]
            row2 = group.loc[i+1]
            if row1['cluster'] == 0 and row2['cluster'] == 1:
                zero_one_pairs.append((article_id, row1['file_index'], row2['file_index']))
            if row1['cluster'] == 1 and row2['cluster'] == 0:
                zero_one_pairs.append((article_id, row1['file_index'], row2['file_index']))

    return zero_one_pairs

# שימוש
pairs = find_zero_one_pairs(final_results_df)
print(f"1&0 Pairs: {len(pairs)}")

def find_one_pairs(df):
    one_pairs = []

    grouped = df.groupby('article_id')
    for article_id, group in grouped:
        group = group.sort_values('file_index').reset_index(drop=True)
        for i in range(len(group) - 1):
            row1 = group.loc[i]
            row2 = group.loc[i+1]
            if row1['cluster'] == 1 and row2['cluster'] == 1:
                one_pairs.append((article_id, row1['file_index'], row2['file_index']))
            if row1['cluster'] == 1 and row2['cluster'] == 1:
                one_pairs.append((article_id, row1['file_index'], row2['file_index']))

    return one_pairs

# שימוש
pairs = find_one_pairs(final_results_df)
print(f"Pairs: {len(pairs)}")

import pandas as pd

def filter_pairs(df):
    filtered_results = []

    # מקבץ לפי article_id כדי לעבור בכל קובץ בנפרד
    grouped = df.groupby('article_id')

    for article_id, group in grouped:
        group = group.sort_values('file_index').reset_index(drop=True)

        # עובר בזוגות רציפים: 0-1, 1-2, 2-3, ...
        for i in range(len(group) - 1):
            row1 = group.loc[i]
            row2 = group.loc[i+1]

            c1, c2 = row1['cluster'], row2['cluster']
            d1, d2 = row1['distance_to_center'], row2['distance_to_center']

            if c1 == 1 and c2 == 1:
                # שניהם 1 -> בחר את זה עם המרחק הקטן ביותר
                chosen = row1 if d1 < d2 else row2
            elif c1 == 0 and c2 == 0:
                # שניהם 0 -> בחר את זה עם המרחק הגדול ביותר
                chosen = row1 if d1 > d2 else row2
            else:
                # קלאסטרים שונים -> בחר את זה שקלאסטר שלו 1
                chosen = row1 if c1 == 1 else row2

            filtered_results.append(chosen)

    return pd.DataFrame(filtered_results).reset_index(drop=True)


# דוגמה לשימוש
filtered_df = filter_pairs(final_results_df)
print(filtered_df)

# הוספת עמודת מספר רץ בשם 'id' מ-0
final_results_df_1 = filtered_df.reset_index(drop=True)  # מבטיח שהאינדקס רציף
final_results_df_1['id'] = final_results_df_1.index

# סינון העמודות הרצויות
filtered_df_final = final_results_df_1[['id', 'file_index']]
filtered_df_final.columns = ['id','real_text_id']
print(filtered_df_final)

df_predictions.to_csv('prediction.csv',index=False)

filtered_df_final.to_csv('prediction_kmeans.csv',index=False)

df_replace = filtered_df_final.copy()
df_replace['real_text_id'] = df_replace['real_text_id'].map({1: 2, 2: 1})
df_replace
df_replace.to_csv('prediction_kmeans2.csv',index=False)
