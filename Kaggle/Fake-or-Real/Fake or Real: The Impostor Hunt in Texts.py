# -*- coding: utf-8 -*-
"""notebookcc02f1c748

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/saharhaimyaccov/notebookcc02f1c748.36524087-96df-43cb-9fad-c10e9d64ae74.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250802/auto/storage/goog4_request%26X-Goog-Date%3D20250802T131028Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D36f01e394c4e364a0ff8b0c633ae5a24fe168a26809e898adaf2a22a0508342fabc38882806d4d3d7b4c25de4728bafc440261dc3ca8a9cf197bb5cffb4098273c7ce8a3cde69390568819dfd3f3ac5e31ce470ae7347b0412fc5c722b4838fd7012b16ca6f872ac757e813eb0abdd6f301e302f03995e4d8650c061bb4d0f52b714b3c84ab5b434fd514a0e500309d85fbf8eb37f4933734e42397e061e72547be5427db14711c77bb808ca46c65cd6a267aaa0fabe555918a9e37ccfe80795124f0cecaab312f8943fe175356c23a2f485e7f07922128dd6a3527762e0649e9a32312798f64b94bee75adda178dc36335f38d7e35518e1add74dd62caf3990
"""

# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE
# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.
import kagglehub
kagglehub.login()

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

fake_or_real_the_impostor_hunt_path = kagglehub.competition_download('fake-or-real-the-impostor-hunt')

print('Data source import complete.')

import numpy as np
import pandas as pd

# load file
labels_df = pd.read_csv('/kaggle/input/fake-or-real-the-impostor-hunt/data/train.csv')

text_true = []
text_fake = []

# for any article in file
for i, row in labels_df.iterrows():
    article_id = row['id']  # ×œ×“×•×’××”: article_0000
    real_id = row['real_text_id']   # 1 ××• 2
    str_article_id = 'article_' + str(i).zfill(4)  # ==> 'article_0005'

    #the pathes pf  articales
    file1_path = f'/kaggle/input/fake-or-real-the-impostor-hunt/data/train/{str_article_id}/file_1.txt'
    file2_path = f'/kaggle/input/fake-or-real-the-impostor-hunt/data/train/{str_article_id}/file_2.txt'


    #read texts
    with open(file1_path, 'r', encoding='utf-8') as f1:
        text1 = f1.read()

    with open(file2_path, 'r', encoding='utf-8') as f2:
        text2 = f2.read()

    #save file by true or fake
    if real_id == 1:
        text_true.append(text1)
        text_fake.append(text2)
    else:
        text_true.append(text2)
        text_fake.append(text1)

    # 94 files exists
    if i == 94:
        break

print(f"ğŸŸ¢ Real texts count: {len(text_true)}")
print(f"ğŸ”´ Fake texts count: {len(text_fake)}")

import re

def clean_and_split(text):
    #Return lowercase words from text without punctuation.
    return re.findall(r'\b\w+\b', text.lower())

zero_pair_text_in_file = []
min_len = min(len(text_true), len(text_fake))


for i in range(len(text_true)):
    true_words = clean_and_split(text_true[i])
    fake_words = clean_and_split(text_fake[i])
    both_words = [word for word in true_words if word in fake_words]
    if (len(both_words) == 0 ):
        zero_pair_text_in_file.append(i+1)
        print(f'ğŸ” Pair {i+1}: {len(both_words)}' )


if zero_pair_text_in_file:
    print(f"\nğŸ˜¶ No common words found in {len(zero_pair_text_in_file)} pairs: {zero_pair_text_in_file}")
else:
    print("\nğŸ‰ All pairs have at least one common word!")

df = pd.DataFrame({'True': text_true,'Fake': text_fake})

df.head()

from wordcloud import WordCloud
import matplotlib.pyplot as plt

def generate_wordcloud(text):
    """
    Generates and displays a word cloud from the given text.

    Args:
        text (str): The input text to visualize.
    """
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title("â˜ï¸ Word Cloud", fontsize=16)
    plt.show()

for num_df_file in [0,1]:
    if num_df_file == 0:
        print(f"ğŸš€ {'-'*20} detect for True file {'-'*20}")
    else:
        print(f"ğŸš¨ {'-'*20} detect for Fake file {'-'*20}")
    for i in range(10):
        print(f'â±ï¸ file {i}')
        sample_text = df.iloc[i, num_df_file]  # ×©×•×¨×” i, ×¢××•×“×” 0
        print(f"ğŸŒŸ Word Cloud for row {i+1}")
        generate_wordcloud(sample_text)

from sklearn.feature_extraction.text import TfidfVectorizer
from joblib import dump, load
def create_vector_from_list(col_values,vectorizer_file_name):
    # create vector from texts
    vectorizer = TfidfVectorizer(stop_words='english')  # auto stop word
    X = vectorizer.fit_transform(col_values)
    X = X.toarray()
    dump(vectorizer, f'{vectorizer_file_name}.joblib')
    return X ,vectorizer,vectorizer.get_feature_names_out()

all_texts = list(text_true) + list(text_fake)  # ×›×œ ×”×˜×§×¡×˜×™× ×‘×™×—×“

vectors_all, vectorizer_all, feature_names_all = create_vector_from_list(all_texts , 'vectorizer_all_1')
vectors_t = vectors_all[:len(text_true), :]
vectors_f = vectors_all[len(text_true):, :]

print(len(vectorizer_all.get_feature_names_out()))

df_all_txt = pd.DataFrame(vectors_all)
print(f"ğŸ§ Total missing values (NaN) in dataframe: {df_all_txt.isnull().sum().sum()} âŒ")

print(df_all_txt.head(2))

print(df_all_txt.shape)  # ×”×ª×•×¦××”: (rows, columns)

df_all_txt.describe()

df_vec_t_f = df_all_txt
print(f"ğŸ” Total missing values (NaN) in dataframe: {df_vec_t_f.isnull().sum().sum()} âŒ")


df_vectors_t_mean = pd.DataFrame(np.mean(vectors_t, axis=1), columns=['mean_vector_t'])
df_vectors_t_std = pd.DataFrame(np.std(vectors_t, axis=1), columns=['std_vector_t'])
df_vectors_f_mean = pd.DataFrame(np.mean(vectors_f, axis=1), columns=['mean_vector_f'])
df_vectors_f_std = pd.DataFrame(np.std(vectors_t, axis=1), columns=['std_vector_f'])

df_true = pd.DataFrame({
    'mean_vector': df_vectors_t_mean['mean_vector_t'],
    'std_vector': df_vectors_t_std['std_vector_t'],
    'label': 'true'
})

df_fake = pd.DataFrame({
    'mean_vector': df_vectors_f_mean['mean_vector_f'],
    'std_vector': df_vectors_f_std['std_vector_f'],
    'label': 'fake'
})

df_vec_t_f_mean = pd.concat([df_true, df_fake], axis=0).reset_index(drop=True)


print(f"ğŸ§ Total missing values (NaN) in dataframe mean: {df_vec_t_f.isnull().sum().sum()} âŒ")
print(df_vec_t_f_mean.iloc[[20,25,34, 40, 102,112,122, 129]])

from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=2, n_init=10, random_state=42)
kmeans.fit(df_all_txt)
df_vec_t_f['cluster'] = kmeans.labels_

print(df_vec_t_f.shape)

print("Number of iterations run:", kmeans.n_iter_)
print("Number of clusters:", kmeans.n_clusters)
print("Random state:", kmeans.random_state)


import matplotlib.pyplot as plt

def plot_clusters(df, kmeans, normalize=True):
    """
    Plot the clustered data points in 2D with cluster centers.

    Args:
        df (pd.DataFrame): Original DataFrame of features.
        kmeans: Fitted KMeans model.
        normalize (bool): Whether the data was normalized (to plot in scaled space).
    """
    if normalize:
        from sklearn.preprocessing import StandardScaler
        scaler = StandardScaler()
        X = scaler.fit_transform(df)
    else:
        X = df.values

    labels = kmeans.labels_
    centers = kmeans.cluster_centers_

    plt.figure(figsize=(8,6))
    scatter = plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', alpha=0.6)
    plt.scatter(centers[:, 0], centers[:, 1], c='red', marker='X', s=200, label='Centers')

    plt.title('KMeans Clusters')
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.legend()
    plt.show()

from sklearn.decomposition import PCA

# ×¦××¦×•× ×œ××¨×—×‘ ×“×•-×××“×™
pca = PCA(n_components=2)
reduced_vectors = pca.fit_transform(vectors_no_cluster)

# ×•××– ×¢×•×©×™× clustering ×¢×œ reduced_vectors

# ×©×™××•×© ×œ×“×•×’××”:
plot_clusters(new_df, kmeans)
print("Cluster Centers Coordinates:")
print(kmeans.cluster_centers_)

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

def train_and_evaluate_random_forest(X, y, test_size=0.2, random_state=42):
    """
    ××××Ÿ ××•×“×œ RandomForestClassifier ×•××—×–×™×¨ ××ª ×“×™×•×§ ×”××•×“×œ ×¢×œ ×§×‘×•×¦×ª ×”×‘×“×™×§×”

    ×¤×¨××˜×¨×™×:
    - X: numpy array ××• DataFrame ×©×œ ×××¤×™×™× ×™×
    - y: numpy array ××• ×¡×“×¨×” ×©×œ ×ª×•×•×™×•×ª (××¡×¤×¨×™×•×ª)
    - test_size: ×—×œ×§ ×”× ×ª×•× ×™× ×©×™×©××© ×œ×‘×“×™×§×” (×‘×¨×™×¨×ª ××—×“×œ 0.2)
    - random_state: ××¡×¤×¨ ×œ×–×¨×¢ ××§×¨××™ (×‘×¨×™×¨×ª ××—×“×œ 42)

    ××—×–×™×¨:
    - accuracy: ×“×™×•×§ ×”××•×“×œ ×¢×œ ×§×‘×•×¦×ª ×”×‘×“×™×§×” (float)
    """
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state, stratify=y
    )
    model = RandomForestClassifier(n_estimators=100, random_state=random_state)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    return accuracy

from xgboost import XGBClassifier

def train_xgboost(X, y, test_size=0.2, random_state=42):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)
    model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=random_state)
    model.fit(X_train, y_train)
    return accuracy_score(y_test, model.predict(X_test))

from lightgbm import LGBMClassifier

def train_lightgbm(X, y, test_size=0.2, random_state=42):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)
    model = LGBMClassifier(random_state=random_state)
    model.fit(X_train, y_train)
    return accuracy_score(y_test, model.predict(X_test))

from sklearn.ensemble import ExtraTreesClassifier

def train_extra_trees(X, y, test_size=0.2, random_state=42):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)
    model = ExtraTreesClassifier(n_estimators=100, random_state=random_state)
    model.fit(X_train, y_train)
    return accuracy_score(y_test, model.predict(X_test))

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam

def train_neural_net(X, y, test_size=0.2, random_state=42, epochs=20, batch_size=32):
    import numpy as np
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import accuracy_score

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)

    model = Sequential([
        Dense(64, activation='relu', input_shape=(X.shape[1],)),
        Dense(32, activation='tanh'),
        Dense(1, activation='sigmoid')
    ])

    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])
    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)
    y_pred = (model.predict(X_test) > 0.5).astype(int)
    return accuracy_score(y_test, y_pred)

from sklearn.preprocessing import LabelEncoder
X = df_vec_t_f_mean.drop(columns = 'label')
y = df_vec_t_f_mean['label']
# ×™×¦×™×¨×ª ××§×•×“×“ ×•×”××¨×”
le = LabelEncoder()
y_encoded = le.fit_transform(y)

accuracy = train_lightgbm(X, y_encoded)
print(f"ğŸ¯ Cross-validated Accuracy: {accuracy:.4f}")

import numpy as np
import pandas as pd
from scipy.stats import entropy


def sigmoid_entropy_like(arr):
    s = np.sum(arr)
    # ×—×™×©×•×‘ ×¡×™×’××•××™×“: 1 / (1 + e^(-s))
    return 1 / (1 + np.exp(-s))


# ×—×™×©×•×‘ ×× ×˜×¨×•×¤×™×” ×œ×›×œ ×©×•×¨×” ×‘××˜×¨×™×¦×” (×•×•×§×˜×•×¨×™× ×˜×§×¡×˜ ×××™×ª×™×™×)
entropies_t = np.array([sigmoid_entropy_like(row) for row in vectors_t])

df_vectors_t_stats = pd.DataFrame({
    'mean_vector': np.mean(vectors_t, axis=1),
    'std_vector': np.std(vectors_t, axis=1),
    'var_vector': np.var(vectors_t, axis=1),
    'median_vector': np.median(vectors_t, axis=1),
    'entropy_vector': entropies_t,
    'label': 'true'
})

# ×—×™×©×•×‘ ×× ×˜×¨×•×¤×™×” ×œ×›×œ ×©×•×¨×” ×‘××˜×¨×™×¦×” (×•×•×§×˜×•×¨×™× ×˜×§×¡×˜ ××–×•×™×¤×™×)
entropies_f = np.array([sigmoid_entropy_like(row) for row in vectors_f])

df_vectors_f_stats = pd.DataFrame({
    'mean_vector': np.mean(vectors_f, axis=1),
    'std_vector': np.std(vectors_f, axis=1),
    'var_vector': np.var(vectors_f, axis=1),
    'median_vector': np.median(vectors_f, axis=1),
    'entropy_vector': entropies_f,
    'label': 'fake'
})

# ××™×—×•×“ ×˜×‘×œ××•×ª True ×•-Fake
df_stats = pd.concat([df_vectors_t_stats, df_vectors_f_stats], axis=0).reset_index(drop=True)

print(df_stats.head())

from itertools import combinations

metrics = [
    'mean_vector',    # ×××•×¦×¢
    'std_vector',     # ×¡×˜×™×™×ª ×ª×§×Ÿ
    'median_vector',  # ×—×¦×™×•×Ÿ
    'var_vector',     # ×©×•× ×•×ª
    'entropy_vector'
]
label_col = 'label'

all_combinations = []

for r in range(1, len(metrics)+1):  # 1 ×¢×“ ×›×œ ×”××“×“×™×
    combs = list(combinations(metrics, r))
    for comb in combs:
        cols = list(comb) + [label_col]
        df_subset = df_stats[cols]
        all_combinations.append((comb, df_subset))

for i in all_combinations:
    print(i[0])

df_stats

# ×”×’×“×¨×ª ×”×¤×•× ×§×¦×™×•×ª ×œ×›×œ ××•×“×œ
model_funcs = {
    'RandomForest': train_and_evaluate_random_forest,
    'XGBoost': train_xgboost,
    'LightGBM': train_lightgbm,
    'ExtraTrees': train_extra_trees,
    'NeuralNetwork':train_neural_net,
}

# ×××’×¨ ×”×ª×•×¦××•×ª ×”×¡×•×¤×™
overall_results = {}

# ×‘×—×™×¨×ª ×¤×™×¦'×¨×™×
selected_features = ['mean_vector'	,'std_vector'	,'var_vector',	'median_vector'	,'entropy_vector']  # ×ª×•×›×œ ×œ×”×•×¡×™×£: 'vector_variance', ×•×›×•'

# ×”×›× ×ª X ×•-y
X = df_stats[selected_features]
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(df_stats['label'])  # ×¢×›×©×™×• y ×™×”×™×” [0, 1]

# ×”×¨×¦×ª ×›×œ ××•×“×œ ×¤×¢× ××—×ª ×‘×œ×‘×“
for model_name, func in model_funcs.items():
    print(f"\nğŸš€ Evaluating model: {model_name}")

    # ×”×¨×¦×ª ×”××•×“×œ
    accuracy = func(X, y)  # ×”×¤×•× ×§×¦×™×” ×©×œ×š ××—×–×™×¨×” ××ª ×”×“×™×•×§ (float)

    # ×©××™×¨×ª ×ª×•×¦××”
    overall_results[model_name] = {
        'accuracy': accuracy,
        'features': selected_features
    }

    print(f"âœ… Model: {model_name} | Accuracy: {accuracy:.4f}")

# ×¡×™×›×•× ×›×•×œ×œ
print("\nğŸ¯ Summary of Results:")
for model_name, result in overall_results.items():
    print(f"{model_name}: Accuracy = {result['accuracy']:.4f} using features {result['features']}")

def get_df_for_combination(all_combinations, target_comb):
    for comb_columns, df_subset in all_combinations:
        if comb_columns == target_comb:
            return df_subset
    return None

target_features = ('mean_vector'	,'std_vector'	,	'median_vector','var_vector','entropy_vector'	) # ×ª×•×›×œ ×œ×”×•×¡×™×£: 'vector_variance', ×•×›×•'
df_target = get_df_for_combination(all_combinations, target_features)

df_target

!pip install optuna

import optuna
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score
import numpy as np

def optimize_random_forest_with_optuna(X, y, n_trials=30):
    """
    ××•×¤×˜×™××™×–×¦×™×™×ª RandomForest ×‘×¢×–×¨×ª Optuna â€“ ×¢×œ ×›×œ ×”×“××˜×” (×‘×œ×™ ×•×œ×™×“×¦×™×” ×—×™×¦×•× ×™×ª).

    ×¤×¨××˜×¨×™×:
    - X: ×××¤×™×™× ×™×
    - y: ×ª×•×•×™×•×ª
    - n_trials: ××¡×¤×¨ × ×™×¡×™×•× ×•×ª (×—×™×¤×•×©×™ ×¤×¨××˜×¨×™×)

    ××—×–×™×¨:
    - best_model: ×”××•×“×œ ×¢× ×”×¤×¨××˜×¨×™× ×”×˜×•×‘×™× ×‘×™×•×ª×¨
    - best_params: ×”×¤×¨××˜×¨×™×
    - best_score: ×”×“×™×•×§ ×”×××•×¦×¢
    """

    def objective(trial):
        # ×”×¦×¢×•×ª ×¤×¨××˜×¨×™×
        n_estimators = trial.suggest_int("n_estimators", 100, 300)
        max_depth = trial.suggest_int("max_depth", 5, 11)
        max_features = trial.suggest_categorical("max_features", ["sqrt", "log2", None])

        model = RandomForestClassifier(
            n_estimators=n_estimators,
            max_depth=max_depth,
            max_features=max_features,
            random_state=42,
            n_jobs=-1
        )

        # ×“×™×•×§ ×¢×œ ×›×œ ×”×“××˜×” (××¤×©×¨ ×œ×”×—×œ×™×£ ×œ-cross_val_score)
        model.fit(X, y)
        preds = model.predict(X)
        acc = accuracy_score(y, preds)
        return acc  # ×“×™×•×§ ×’×‘×•×” ×™×•×ª×¨ = ×˜×•×‘ ×™×•×ª×¨

    study = optuna.create_study(direction="maximize")
    study.optimize(objective, n_trials=n_trials)

    best_params = study.best_params
    best_score = study.best_value

    # ××™××•×Ÿ ×”××•×“×œ ×”×¡×•×¤×™ ×¢× ×”×¤×¨××˜×¨×™× ×”×˜×•×‘×™× ×‘×™×•×ª×¨
    best_model = RandomForestClassifier(**best_params, random_state=42, n_jobs=-1)
    best_model.fit(X, y)

    return best_model, best_params, best_score

# ×™×¦×™×¨×ª X ×•-y (× × ×™×— ×©×›×‘×¨ ×©×˜×—×ª ×•×§×˜×•×¨×™×)


# ×”×¨×¦×ª Optuna
best_model, best_params, best_acc = optimize_random_forest_with_optuna(X, y, n_trials=40)

print("ğŸ† Best RandomForest Model:")
print(f"Params: {best_params}")
print(f"Accuracy: {best_acc:.4f}")

from pprint import pprint
pprint(best_model.get_params())
print('-'*70)
import numpy as np
print(f"Most used feature index: {np.argmax(best_model.feature_importances_)}")
print('-'*70)
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.bar(range(len(best_model.feature_importances_)), best_model.feature_importances_)
plt.title("Feature Importances")
plt.xlabel("Feature Index")
plt.ylabel("Importance")
plt.show()
import joblib
joblib.dump(best_model, "best_random_forest_model.pkl")

import os

folder_path = '/kaggle/input/fake-or-real-the-impostor-hunt/data/test'
lab = {}

for root, dirs, files in os.walk(folder_path):
    # ×©×•×œ×£ ××ª ×©× ×”×ª×§×™×” ××ª×•×š ×”× ×ª×™×‘ (×”×ª×§×™×” ×”× ×•×›×—×™×ª)
    folder_name = os.path.basename(root)

    for filename in files:
        file_path = os.path.join(root, filename)
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # ×× ×”××¤×ª×— ×§×™×™×, ××•×¡×™×£ ×œ×¨×©×™××”, ××—×¨×ª ×™×•×¦×¨ ×¨×©×™××” ×—×“×©×”
        if folder_name not in lab:
            lab[folder_name] = []
        lab[folder_name].append(content)

import numpy as np
import pandas as pd

# ×©× ×”××××¨ ×œ×‘×“×™×§×”
article_id = 'article_0000'

# ×ª×›×•× ×•×ª ×©× ×‘×—×¨×• ×¢×‘×•×¨ ×”××•×“×œ
target_features = selected_features

# ×”×“×¤×¡×ª ××¡×¤×¨ ×”×˜×§×¡×˜×™× ×‘××××¨
num_texts = len(lab[article_id])
print(f"Number of texts in {article_id}: {num_texts}")

# ×©×œ×™×¤×ª ×©× ×™ ×”×˜×§×¡×˜×™× ×”×¨××©×•× ×™×
text1 = lab[article_id][0]
text2 = lab[article_id][1]

# ×™×¦×™×¨×ª ×•×§×˜×•×¨×™× ××”×˜×§×¡×˜×™×
vector1, _, _ = create_vector_from_list([text1],'vectorizer_txt1')
vector2, _, _ = create_vector_from_list([text2],'vectorizer_txt2')

# ×”×ª×××ª ××‘× ×” ×—×“-×××“×™
vec1 = vector1 if len(vector1.shape) == 1 else vector1[0]
vec2 = vector2 if len(vector2.shape) == 1 else vector2[0]

# ×—×™×©×•×‘ ×ª×›×•× ×•×ª ×œ×˜×§×¡×˜ ×”×¨××©×•×Ÿ
features1 = {
    'mean_vector': np.mean(vec1),
    'std_vector': np.std(vec1),
    'median_vector': np.median(vec1),
    'var_vector': np.var(vec1),
    'entropy_vector': sigmoid_entropy_like(vec1)
}

# ×—×™×©×•×‘ ×ª×›×•× ×•×ª ×œ×˜×§×¡×˜ ×”×©× ×™
features2 = {
    'mean_vector': np.mean(vec2),
    'std_vector': np.std(vec2),
    'median_vector': np.median(vec2),
    'var_vector': np.var(vec2),
    'entropy_vector': sigmoid_entropy_like(vec2)
}

# ×™×¦×™×¨×ª ×˜×‘×œ××•×ª × ×ª×•× ×™×
df1 = pd.DataFrame([features1])
df2 = pd.DataFrame([features2])

# ×—×™×–×•×™ ×¢×‘×•×¨ ×˜×§×¡×˜ ×¨××©×•×Ÿ
prob1 = best_model.predict_proba(df1[target_features])[0][1]
pred1 = int(prob1 > 0.5)

# ×—×™×–×•×™ ×¢×‘×•×¨ ×˜×§×¡×˜ ×©× ×™
prob2 = best_model.predict_proba(df2[target_features])[0][1]
pred2 = int(prob2 > 0.5)

# ×”×“×¤×¡×ª ×ª×•×¦××•×ª
print(f"\nText 1: Prediction probability: {prob1:.4f} | Binary prediction: {pred1}")
print(f"Text 2: Prediction probability: {prob2:.4f} | Binary prediction: {pred2}")

print(len(vectorizer_all.get_feature_names_out()))

from joblib import load
import numpy as np
from sklearn.base import ClusterMixin

def predict_and_distance(text: str, vectorizer: str, model: ClusterMixin) -> tuple[int, float]:
    """
    Predict cluster and compute Euclidean distance from cluster center for a given text.

    Args:
        text (str): Input text to classify.
        vectorizer_path (str): Path to saved vectorizer (.joblib).
        model (ClusterMixin): Pre-trained clustering model (e.g., KMeans).

    Returns:
        tuple[int, float]: (predicted_cluster, distance_to_cluster_center)
    """

    vec = vectorizer.transform([text])

    cluster = model.predict(vec)[0]
    center = model.cluster_centers_[cluster]
    distance = np.linalg.norm(vec.toarray() - center)

    return cluster, distance

# ×˜×¢×™× ×ª ×”××•×“×œ ×¤×¢× ××—×ª ××—×•×¥ ×œ×¤×•× ×§×¦×™×” (×œ××©×œ ×‘×ª×—×™×œ×ª ×”×¡×§×¨×™×¤×˜)
# from joblib import load
# k_mean_model_1 = load('kmeans_model.joblib

# ×©×™××•×©
cluster_id, dist = predict_and_distance(text1, vectorizer_all, kmeans)
cluster_id2, dist2 = predict_and_distance(text2, vectorizer_all, kmeans)

print(f"Cluster: {cluster_id}")
print(f"Distance from center: {dist:.6f}")
print(f"Cluster: {cluster_id2}")
print(f"Distance from center: {dist2:.6f}")
if dist>dist2:
    print('ğŸ“˜ text file 1 article 1 is Real !')
    print('ğŸ“• text file 1 article 2 is Fake !')
else:
    print('ğŸ“• text file 1 article 2 is Real !')
    print('ğŸ“˜ text file 1 article 1 is Fake !')

results = []

for article_id, texts in lab.items():
    for idx, text in enumerate(texts):
        try:
            # ×™×¦×™×¨×ª ×•×§×˜×•×¨ ×˜×§×¡×˜ ×¢× ×”-vectorizer ×©×©××¨×ª (×œ×“×•×’××”)
            vec = vectorizer_all.transform([text])

            # ×—×™×–×•×™ ××©×›×•×œ ×¢× ××•×“×œ KMeans
            cluster = kmeans.predict(vec)[0]

            # ×—×™×©×•×‘ ××¨×—×§ ××•×§×œ×™×“×™ ×œ××¨×›×– ×”××©×›×•×œ
            center = kmeans.cluster_centers_[cluster]
            distance = np.linalg.norm(vec.toarray() - center)

            # ×”×•×¡×¤×ª ×ª×•×¦××”
            results.append({
                'article_id': article_id,
                'file_index': idx + 1,
                'cluster': cluster,
                'distance_to_center': distance
            })

        except Exception as e:
            print(f"Error processing article {article_id}, file_index {idx + 1}: {e}")
            # ×‘××§×¨×” ×©×œ ×©×’×™××” â€“ ××¤×©×¨ ×œ×©×™× ×¢×¨×›×™× ×‘×¨×™×¨×ª ××—×“×œ
            results.append({
                'article_id': article_id,
                'file_index': idx + 1,
                'cluster': -1,
                'distance_to_center': np.nan
            })

# ×™×¦×™×¨×ª DataFrame ×¡×•×¤×™
final_results_df = pd.DataFrame(results)
print(final_results_df.head())

final_results_df

def find_zero_one_pairs(df):
    zero_one_pairs = []

    grouped = df.groupby('article_id')
    for article_id, group in grouped:
        group = group.sort_values('file_index').reset_index(drop=True)
        for i in range(len(group) - 1):
            row1 = group.loc[i]
            row2 = group.loc[i+1]
            if row1['cluster'] == 0 and row2['cluster'] == 1:
                zero_one_pairs.append((article_id, row1['file_index'], row2['file_index']))
            if row1['cluster'] == 1 and row2['cluster'] == 0:
                zero_one_pairs.append((article_id, row1['file_index'], row2['file_index']))

    return zero_one_pairs

# ×©×™××•×©
pairs = find_zero_one_pairs(final_results_df)
print(f"1&0 Pairs: {len(pairs)}")

def find_one_pairs(df):
    one_pairs = []

    grouped = df.groupby('article_id')
    for article_id, group in grouped:
        group = group.sort_values('file_index').reset_index(drop=True)
        for i in range(len(group) - 1):
            row1 = group.loc[i]
            row2 = group.loc[i+1]
            if row1['cluster'] == 1 and row2['cluster'] == 1:
                one_pairs.append((article_id, row1['file_index'], row2['file_index']))
            if row1['cluster'] == 1 and row2['cluster'] == 1:
                one_pairs.append((article_id, row1['file_index'], row2['file_index']))

    return one_pairs

# ×©×™××•×©
pairs = find_one_pairs(final_results_df)
print(f"Pairs: {len(pairs)}")

import pandas as pd

def filter_pairs(df):
    filtered_results = []

    # ××§×‘×¥ ×œ×¤×™ article_id ×›×“×™ ×œ×¢×‘×•×¨ ×‘×›×œ ×§×•×‘×¥ ×‘× ×¤×¨×“
    grouped = df.groupby('article_id')

    for article_id, group in grouped:
        group = group.sort_values('file_index').reset_index(drop=True)

        # ×¢×•×‘×¨ ×‘×–×•×’×•×ª ×¨×¦×™×¤×™×: 0-1, 1-2, 2-3, ...
        for i in range(len(group) - 1):
            row1 = group.loc[i]
            row2 = group.loc[i+1]

            c1, c2 = row1['cluster'], row2['cluster']
            d1, d2 = row1['distance_to_center'], row2['distance_to_center']

            if c1 == 1 and c2 == 1:
                # ×©× ×™×”× 1 -> ×‘×—×¨ ××ª ×–×” ×¢× ×”××¨×—×§ ×”×§×˜×Ÿ ×‘×™×•×ª×¨
                chosen = row1 if d1 < d2 else row2
            elif c1 == 0 and c2 == 0:
                # ×©× ×™×”× 0 -> ×‘×—×¨ ××ª ×–×” ×¢× ×”××¨×—×§ ×”×’×“×•×œ ×‘×™×•×ª×¨
                chosen = row1 if d1 > d2 else row2
            else:
                # ×§×œ××¡×˜×¨×™× ×©×•× ×™× -> ×‘×—×¨ ××ª ×–×” ×©×§×œ××¡×˜×¨ ×©×œ×• 1
                chosen = row1 if c1 == 1 else row2

            filtered_results.append(chosen)

    return pd.DataFrame(filtered_results).reset_index(drop=True)


# ×“×•×’××” ×œ×©×™××•×©
filtered_df = filter_pairs(final_results_df)
print(filtered_df)

# ×”×•×¡×¤×ª ×¢××•×“×ª ××¡×¤×¨ ×¨×¥ ×‘×©× 'id' ×-0
final_results_df_1 = filtered_df.reset_index(drop=True)  # ××‘×˜×™×— ×©×”××™× ×“×§×¡ ×¨×¦×™×£
final_results_df_1['id'] = final_results_df_1.index

# ×¡×™× ×•×Ÿ ×”×¢××•×“×•×ª ×”×¨×¦×•×™×•×ª
filtered_df_final = final_results_df_1[['id', 'file_index']]
filtered_df_final.columns = ['id','real_text_id']
print(filtered_df_final)

df_predictions.to_csv('prediction.csv',index=False)

filtered_df_final.to_csv('prediction_kmeans.csv',index=False)

df_replace = filtered_df_final.copy()
df_replace['real_text_id'] = df_replace['real_text_id'].map({1: 2, 2: 1})
df_replace
df_replace.to_csv('prediction_kmeans2.csv',index=False)
